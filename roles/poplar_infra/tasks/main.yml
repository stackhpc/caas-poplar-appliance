# Modified by Graphcore Ltd.
# Convert the floating IP id to an address for Terraform
- block:
    - name: Look up floating ip
      stackhpc.terraform.os_floating_ip_info:
        floating_ip: "{{ cluster_floating_ip }}"
      register: cluster_floating_ip_info

    - name: Set floating IP address fact
      set_fact:
        cluster_floating_ip_address: "{{ cluster_floating_ip_info.floating_ip.floating_ip_address }}"
  when:
    - cluster_floating_ip is defined
    - cluster_floating_ip

- name: Make Terraform project directory
  file:
    path: "{{ terraform_project_path }}"
    state: directory

- name: Write backend configuration
  copy:
    content: |
      terraform {
        backend "{{ terraform_backend_type }}" { }
      }
    dest: "{{ terraform_project_path }}/backend.tf"

# Patching in this appliance is implemented as a switch to a new base image
# So unless explicitly patching, we want to use the same image as last time
# To do this, we query the previous Terraform state before updating
- block:
    - name: Get previous Terraform state
      stackhpc.terraform.terraform_output:
        binary_path: "{{ terraform_binary_path }}"
        project_path: "{{ terraform_project_path }}"
        backend_config: "{{ terraform_backend_config }}"
      register: cluster_infra_terraform_output

    - name: Extract image from Terraform state
      set_fact:
        cluster_previous_image: "{{ cluster_infra_terraform_output.outputs.cluster_image.value }}"
      when: '"cluster_image" in cluster_infra_terraform_output.outputs'
  when:
    - terraform_state == "present"
    - cluster_upgrade_system_packages is not defined or not cluster_upgrade_system_packages

# SYSOP-1087 - make deletion more robust
- block:
    - name: Get previous Terraform state
      stackhpc.terraform.terraform_output:
        binary_path: "{{ terraform_binary_path }}"
        project_path: "{{ terraform_project_path }}"
        backend_config: "{{ terraform_backend_config }}"
      register: cluster_infra_terraform_output

    - name: Extract user rnic network ID from state
      set_fact:
        user_rnic_net_id: "{{ cluster_infra_terraform_output.outputs.user_rnic_net_id.value }}"
      when: '"user_rnic_net_id" in cluster_infra_terraform_output.outputs'

    - name: Extract user rnic subnet ID from state
      set_fact:
        user_rnic_subnet_id: "{{ cluster_infra_terraform_output.outputs.user_rnic_subnet_id.value }}"
      when: '"user_rnic_subnet_id" in cluster_infra_terraform_output.outputs'
  when:
    - terraform_state == "absent"

- name: Template Terraform files into project directory
  template:
    src: "{{ tf_var_cluster }}.j2"
    dest: "{{ terraform_project_path }}/{{ tf_var_cluster }}"
  loop:
    - outputs.tf
    - providers.tf
    - resources.tf
  loop_control:
    loop_var: tf_var_cluster

- name: Get resources.tf for debug
  ansible.builtin.slurp:
    src: "{{ terraform_project_path }}/resources.tf"
  register: slurpfile
  tags: [debug, never]

- name: Debug resources.tf file
  ansible.builtin.debug:
    msg: "{{ slurpfile['content'] | b64decode }}"
  tags: [debug, never]

- name: Test if appliance VM exists
  ansible.builtin.shell:
    cmd: openstack server list --name {{cluster_name}}
  register: vm_query
  when: terraform_state == 'absent'

- name: Provision infrastructure
  community.general.terraform:
    binary_path: "{{ terraform_binary_path }}"
    project_path: "{{ terraform_project_path }}"
    state: "{{ terraform_state }}"
    backend_config: "{{ terraform_backend_config }}"
    force_init: yes
    init_reconfigure: yes
  register: terraform_provision
  when: (terraform_state == 'present') or (terraform_state == 'absent' and "cluster_name in vm_query.stdout")

- name: Populate facts from Terraform outputs
  set_fact:
    # cluster_access_ip: "{{ terraform_provision.outputs.cluster_access_ip.value }}"
    cluster_user: "{{ terraform_provision.outputs.cluster_user.value }}"
    cluster_flavour_name: "{{ terraform_provision.outputs.cluster_flavour_name.value }}"
    # appliance_ctrl_ip: "{{ terraform_provision.outputs.appliance_ctrl_ip.value }}"
  when:
    - terraform_state == "present"

# TODO: and why does a re-run of the create trigger the volumes and instance to be rebuilt?

- name: Wait for Monitoring to become available
  uri:
    url: "http://{{ zenith_fqdn_monitoring }}"
    method: GET
    follow_redirects: safe
  register: monitoring_uri
  changed_when: false
  failed_when: >-
    monitoring_uri.status < 0 or
    monitoring_uri.status == 404 or
    monitoring_uri.status >= 500
  retries: 60
  delay: 10
  # 404 and 503 are the statuses that are seen when the Zenith service is not ready yet
  # An SSL error is indicated as -1, which will occur while cert-manager fetches certificates
  until: monitoring_uri.status not in [404, 503, -1]
  when: (cluster_state | default('present')) == 'present'

- name: Wait for webconsole to become available
  uri:
    url: "http://{{ zenith_fqdn_webconsole }}"
    method: GET
    follow_redirects: safe
  register: webconsole_uri
  changed_when: false
  failed_when: >-
    webconsole_uri.status < 0 or
    webconsole_uri.status == 404 or
    webconsole_uri.status >= 500
  retries: 60
  delay: 10
  # 404 and 503 are the statuses that are seen when the Zenith service is not ready yet
  # An SSL error is indicated as -1, which will occur while cert-manager fetches certificates
  until: webconsole_uri.status not in [404, 503, -1]
  when: (cluster_state | default('present')) == 'present'
